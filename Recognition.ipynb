{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-3-B5ehqMWhK"},"outputs":[],"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.resnet import ResNet50\n","from keras.applications.resnet import ResNet101\n","from keras.applications.resnet import ResNet152\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1437,"status":"ok","timestamp":1642369058975,"user":{"displayName":"edo lj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08764512643524898872"},"user_tz":-60},"id":"a_AZQB4ON0j6","outputId":"fcd11319-b4f7-4077-92cd-a7d942fbdffb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":312,"status":"ok","timestamp":1642369065567,"user":{"displayName":"edo lj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08764512643524898872"},"user_tz":-60},"id":"LI5r23T3YUFl","outputId":"c0309a72-b994-47f0-dc4a-afbb62e0021c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive\n"]}],"source":["%cd gdrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1642369068536,"user":{"displayName":"edo lj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08764512643524898872"},"user_tz":-60},"id":"WjJKneukYZWk","outputId":"c845e9b7-c8d3-440b-d886-f370d702254d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/MyDrive\n"]}],"source":["%cd MyDrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1642369070997,"user":{"displayName":"edo lj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08764512643524898872"},"user_tz":-60},"id":"BK6gz9dRTqit","outputId":"44880c7e-8847-434d-bbb3-8830200c82fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["100\n","/content/gdrive/MyDrive/trainSet\n"]}],"source":["img_size = 128\n","\n","train_path = 'trainSet'\n","#test_path = 'testSet'\n","\n","folders = glob('trainSet/*')\n","print(len(folders))\n","\n","import pathlib,PIL\n","data_dir = pathlib.Path('/content/gdrive/MyDrive/trainSet')\n","print(data_dir)\n","#ears = list(data_dir.glob('077/*'))\n","#print(ears)\n","#PIL.Image.open(str(ears[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rirKzhANUQJJ"},"outputs":[],"source":["# add preprocessing layer to the front of VGG\n","#vgg = VGG16(input_shape=(128,128,3), weights='imagenet', include_top=False)\n","#resnet50 = ResNet50(input_shape=(128,128,3), weights='imagenet', include_top=False)\n","#model = Sequential()\n","model = Sequential([\n","    layers.RandomFlip(\"horizontal\", input_shape=(img_size, img_size, 3)),\n","    layers.RandomRotation(0.2),\n","    layers.RandomZoom(0.2),\n","  ])\n","\n","# change ResNet50, ResNet101, ResNet152, VGG16\n","pretrained_model= tf.keras.applications.ResNet50(include_top=False,\n","                   input_shape=(img_size,img_size,3),\n","                   pooling='max',\n","                   classes=100,\n","                   weights='imagenet')\n","for layer in pretrained_model.layers:\n","        layer.trainable=False\n","\n","model.add(pretrained_model)\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu'))\n","model.add(Dense(100, activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-UQQgueOw8C"},"outputs":[],"source":["# Apply data augmentation\n","#data_augmentation = keras.Sequential(\n","#  [\n","#    layers.RandomFlip(\"horizontal\", input_shape=(img_size, img_size, 3)),\n","#    layers.RandomRotation(0.1),\n","#    layers.RandomZoom(0.1),\n","#  ]\n","#)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJrC7L4UWymi"},"outputs":[],"source":["# don't train existing weights\n","#for layer in vgg.layers:\n","#  layer.trainable = False\n","#for layer in resnet50.layers:\n","#  layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmPaOY8pW0QD"},"outputs":[],"source":["#x = Flatten()(vgg.output)\n","#x = Flatten()(resnet50.output)\n","\n","#prediction = Dense(len(folders), activation='softmax')(x)\n","\n","# create a model object\n","#model = Model(inputs=vgg.input, outputs=prediction)\n","#model = Model(inputs=resnet50.input, outputs=prediction)\n","\n","# view the structure of the model\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgyZYlmhXDhK"},"outputs":[],"source":["# tell the model what cost and optimization method to use\n","model.compile(\n","  loss='sparse_categorical_crossentropy',\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1642369089032,"user":{"displayName":"edo lj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08764512643524898872"},"user_tz":-60},"id":"rX6RBrHoFCrg","outputId":"fb05dc36-5c21-460f-9e24-542f92d0019d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 750 files belonging to 100 classes.\n","Using 675 files for training.\n"]}],"source":["batch_size=32\n","training_set = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.1,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_size, img_size),\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":300,"status":"ok","timestamp":1642369090753,"user":{"displayName":"edo lj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08764512643524898872"},"user_tz":-60},"id":"CUINxvByFNE5","outputId":"edb05d02-da0c-49e8-afe4-4bb7ba07923f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 750 files belonging to 100 classes.\n","Using 75 files for validation.\n"]}],"source":["test_set = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.1,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_size, img_size),\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hs6ncZOBXMAT"},"outputs":[],"source":["#from keras.preprocessing.image import ImageDataGenerator\n","\n","#train_datagen = ImageDataGenerator(rescale = 1./255,\n","#                                   shear_range = 0.1,\n","#                                   zoom_range = 0.1,\n","#                                   horizontal_flip = True,\n","#                                   vertical_flip = True)\n","\n","#test_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","# Generating the Training Data\n","#training_set = train_datagen.flow_from_directory(\n","#        train_path,\n","#        target_size=(64, 64),\n","#        batch_size=32,\n","#        class_mode='categorical')\n"," \n"," \n","# Generating the Testing Data\n","#test_set = test_datagen.flow_from_directory(\n","#        train_path,\n","#        target_size=(64, 64),\n","#        batch_size=32,\n","#        class_mode='categorical')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lyz7O132C0PY"},"outputs":[],"source":["#print(test_set.class_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xasVDcGDv2ys"},"outputs":[],"source":["#from keras import callbacks\n","#earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n","#                                        mode =\"min\", patience = 20, \n","#                                        restore_best_weights = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSpyr0fBwXrx"},"outputs":[],"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","filepath = \"modelCheckpoint/checkpoint-{epoch:02d}-{val_accuracy:.2f}.h5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuOn4pRpXzfP"},"outputs":[],"source":["# fit the model\n","#r = model.fit(\n","#  training_set,\n","#  validation_data=test_set,\n","#  epochs=50,\n","#  steps_per_epoch=len(training_set),\n","#  validation_steps=len(test_set),\n","#  callbacks =[checkpoint]\n","#)\n","r = model.fit(\n","  training_set,\n","  validation_data=test_set,\n","  epochs=30,\n","  callbacks =[checkpoint]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BV4jhTXxZtJA"},"outputs":[],"source":["# loss\n","plt.plot(r.history['loss'], label='train loss')\n","plt.plot(r.history['val_loss'], label='val loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQIGmH3YZvkm"},"outputs":[],"source":["# accuracies\n","plt.plot(r.history['accuracy'], label='train acc')\n","plt.plot(r.history['val_accuracy'], label='val acc')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WEJQ1xAaQGR"},"outputs":[],"source":["from keras.models import load_model\n","\n","#model.save('vgg_best_model.h5')\n","model.save('resnet50_best_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSJhYGCQd_wY"},"outputs":[],"source":["#https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c\n","\n","from keras.preprocessing import image\n","img = image.load_img(\"testSet/001/0004.png\",target_size=(img_size,img_size))\n","img = np.asarray(img)\n","plt.imshow(img)\n","img = np.expand_dims(img, axis=0)\n","\n","#from keras.models import load_model\n","#saved_model = load_model(\"resnet50_best_model.h5\")\n","\n","output = model.predict(img)\n","print(output)\n","\n","pred = np.argmax(output) + 1\n","print(pred)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOydQe8e3Tfy5PyALQymUoh","collapsed_sections":[],"name":"Recognition.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
